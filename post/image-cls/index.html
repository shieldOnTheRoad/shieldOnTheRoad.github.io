<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>A Survey for Image Classification - shield&#39;s Blogs</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="shield" />
  <meta name="description" content="图像分类算法综述，结合PASCAL VOC、ImageNet两个数据集介绍近20年来有关图像分类的相关技术。包含：手工特征、特征学习两种主要方法。
" />

  <meta name="keywords" content="learning, disscussion" />






<meta name="generator" content="Hugo 0.35" />


<link rel="canonical" href="http://shieldOnTheRoad.github.io/post/image-cls/" />

<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">







<link href="/dist/even.min.css?v=3.0.0" rel="stylesheet">
<link href="/lib/fancybox/jquery.fancybox-3.1.20.min.css" rel="stylesheet">




<meta property="og:title" content="A Survey for Image Classification" />
<meta property="og:description" content="图像分类算法综述，结合PASCAL VOC、ImageNet两个数据集介绍近20年来有关图像分类的相关技术。包含：手工特征、特征学习两种主要方法。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://shieldOnTheRoad.github.io/post/image-cls/" />



<meta property="article:published_time" content="2018-03-21T12:47:36&#43;08:00"/>

<meta property="article:modified_time" content="2018-03-26T10:47:36&#43;08:00"/>











<meta itemprop="name" content="A Survey for Image Classification">
<meta itemprop="description" content="图像分类算法综述，结合PASCAL VOC、ImageNet两个数据集介绍近20年来有关图像分类的相关技术。包含：手工特征、特征学习两种主要方法。">


<meta itemprop="datePublished" content="2018-03-21T12:47:36&#43;08:00" />
<meta itemprop="dateModified" content="2018-03-21T12:47:36&#43;08:00" />
<meta itemprop="wordCount" content="5725">



<meta itemprop="keywords" content="image-cls," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="A Survey for Image Classification"/>
<meta name="twitter:description" content="图像分类算法综述，结合PASCAL VOC、ImageNet两个数据集介绍近20年来有关图像分类的相关技术。包含：手工特征、特征学习两种主要方法。"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Shield&#39;s Blogs</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Shield&#39;s Blogs</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about">About</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">A Survey for Image Classification</h1>

      <div class="post-meta">
        <span class="post-time"> 2018-03-21 </span>
        <div class="post-category">
            
              <a href="/categories/research/"> Research </a>
            
          </div>
        
        
      </div>
    </header>

    
    

    
    <div class="post-content">
      <p><code>图像分类</code>算法综述，结合<code>PASCAL VOC</code>、<code>ImageNet</code>两个数据集介绍近20年来有关<code>图像分类</code>的相关技术。包含：<code>手工特征</code>、<code>特征学习</code>两种主要方法。</p>

<p>
<br></p>

<h2 id="1-概述">#1 <strong>概述</strong></h2>

<p>&emsp;&emsp;<code>图像分类</code>任务即是识别一张图片是否含有某类物体，有关<code>图像分类</code>算法的研究已经有五十多年历史，随着一些重要数据（e.g., <code>PASCAL VOC</code>、<code>ImageNet</code>）的相继推出，<code>图像分类</code>算法有了长足的发展。研究内容主要集中于如何对图像的特征进行描述。一般来说，<code>图像分类</code>算法通过<code>手工特征</code>或者<code>特征学习方法</code>对整幅图像进行全局描述，然后利用分类器判断是否存在某类物体 <sup><a href="#ref01">[1]</a></sup>。</p>

<h3 id="1-1-手工特征"><strong>1.1 手工特征</strong></h3>

<p>&emsp;&emsp;基于词袋模型（Bag of Words）的图像分类算法是提取<code>手工特征</code>的主要算法。图像分类里的词袋模型和文本挖掘中的词袋模型非常相似，通过构建一个通用的词表（图像对应由图像局部特征构建的词表、文本对应由文本词汇构成的词表），统计每一个训练数据在词表中出现的个数或差值等等形成表达特征。基于词袋模型的图像分类算法主要包括以下四个方面：1）底层特征提取；2）特征编码；3）特征汇聚；4）分类器设计。</p>

<h4 id="emsp-emsp-1-底层特征提取-br"><strong>&emsp;&emsp;1）底层特征提取</strong><br></h4>

<p>&emsp;&emsp;底层特征是图像分类的第一步，底层特征提取方式有两种：一种是基于兴趣点检测，另一种是采用密集提取的方式。兴趣点检测算法通过某种准则选择具有明确定义的、局部纹理特征比较明显的像素点、边缘、角点、区块等，并且通常能够获得一定的几何不变性，从而可以在较小的开销下得到更有意义的表达，最常用的兴趣点检测算子有Harris角点检测子、Shi-Tomasi角点检测子、FAST（Featuress from Accelerated Segment Test）算子、LoG（Laplacian of Gaussian）、DoG（Difference of Gaussian）等。</p>

<p>&emsp;&emsp;近年来图像分类领域使用更多的则是密集提取的方式，从图像中按固定的步长、尺度提取出大量的局部特征描述，大量的局部描述尽管具有更高的冗余度，但信息更加丰富，后面再使用词袋模型进行有效表达后通常可得到比兴趣点检测更好的性能。常用的局部特征包括SIFT <sup><a href="#ref12">[12]</a></sup>（Scale Invariant Feature Transform，尺度不变特征转换）、HOG <sup><a href="#ref11">[11]</a></sup>（Histogram of Oriented Gradient，方向梯度直方图）等。<code>PASCAL VOC</code>竞赛历年最好的图像分类算法都采用了多种特征，采样方式上密集提取与兴趣点检测相结合，底层特征描述也采用了多种特征描述子，这样做的好处是，在底层特征提取阶段，通过提取到大量的冗余特征，最大限度的对图像进行底层描述，防止丢失过多的有用信息，这些底层描述中的冗余信息主要靠后面的特征编码和特征汇聚得到抽象和简并。</p>

<h4 id="emsp-emsp-2-特征编码"><strong>&emsp;&emsp;2）特征编码</strong></h4>

<p>&emsp;&emsp;密集提取的底层特征中包含了大量的冗余与噪声，为提高特征表达的鲁棒性，需要使用一种特征变换算法对底层特征进行编码，从而获得更具区分性、更加鲁棒的特征表达，这一步对图像分类的性能具有至关重要的作用，因而大量的研究工作都集中在寻找更加强大的特征编码方法，重要的特征编码算法包括向量量化编码、稀疏编码、局部线性约束编码、VLAD <sup><a href="#ref13">[13]</a></sup>（Vector of Locally Aggregated Descriptors）向量编码、Fisher向量编码 <sup><a href="#ref14">[14]</a></sup>等。</p>

<h4 id="emsp-emsp-3-特征汇聚"><strong>&emsp;&emsp;3）特征汇聚</strong></h4>

<p>&emsp;&emsp;空间特征汇聚是特征编码后进行的特征集整合操作，通过对编码后的特征，每一维都取其最大值或者平均值，得到一个紧致的特征向量作为图像的特征表达。这一步得到的图像表达可以获得一定的特征不变性，同时也避免了使用特征集进行图像表达的高额代价。最大值汇聚在绝大部分情况下的性能要优于平均值汇，其在图像分类中使用最为广泛。由于图像通常具有极强的空间结构约束，空间金字塔匹配SPM <sup><a href="#ref15">[15]</a></sup>（Spatial Pyramid Matching）提出将图像均匀分块，然后每个区块里面单独做特征汇聚操作并将所有特征向量拼接起来作为图像最终的特征表达。</p>

<h4 id="emsp-emsp-4-分类器设计"><strong>&emsp;&emsp;4）分类器设计</strong></h4>

<p>&emsp;&emsp;使用支持向量机等分类器进行分类。从图像提取到特征表达之后，一张图像可以使用一个固定维度的向量进行描述，接下来就是学习一个分类器对图像进行分类．这个时候可以选择的分类器就很多了，常用的分类器有支持向量机、犓近邻、神经网络、随机森林等。基于最大化边界的支持向量机是使用最为广泛的分类器之一，在图像分类任务上性能很好，特别是使用了核方法的支持向量。</p>

<h3 id="1-2-特征学习"><strong>1.2 特征学习</strong></h3>

<p>&emsp;&emsp;最近几年，深度学习模型逐渐成为图像分类的首选算法，其基本思想是通过有监督或者无监督的方式学习层次化的特征表达，来对图像特征进行从底层图像语义到高层图像语义的描述。主流的深度学习模型主要包括自动编码器（Auto Encoder）、受限波尔兹曼机RBM （Restricted Boltzmann Machine）、深度信念网络DBN （Deep Belief Nets）卷积神经网络CNN （Convolutional Neural Networks）。从最近几年超大规模图像分类比赛<code>ImageNet</code>的冠军模型可以发现，卷积神经网络正在成为最主流的图像分类算法。2012年Hinton等人提出<code>AlexNet</code>，首次颠覆了以<code>手工特征</code>为核心的图像分类算法。<code>ImageNet</code>图像分类竞赛后续的几年中，相继涌现出分类性能良好的卷积神经网络，比如：<code>ZFNet</code>、<code>VGGNet</code>、<code>GoogleNet</code>、<code>Resnet</code>、<code>SENet</code>等。</p>

<p><br></p>

<h2 id="2-pascal-voc图像分类算法">#2 <strong>PASCAL VOC图像分类算法</strong></h2>

<table>
<thead>
<tr>
<th align="center">年份</th>
<th align="center">底层特征</th>
<th align="center">特征编码</th>
<th align="center">空间约束</th>
<th align="center">分类器</th>
<th align="center">融合</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">2005</td>
<td align="center">密集 SIFT</td>
<td align="center">向量量化</td>
<td align="center">无</td>
<td align="center">线性SVM</td>
<td align="center">特征拼接</td>
</tr>

<tr>
<td align="center">2006</td>
<td align="center">兴趣点检测＋密集提取</td>
<td align="center">向量量化</td>
<td align="center">SPM</td>
<td align="center">两层核SVM</td>
<td align="center">两层融合</td>
</tr>

<tr>
<td align="center">2007</td>
<td align="center">密集＋兴趣点，多特征</td>
<td align="center">向量量化</td>
<td align="center">SPM</td>
<td align="center">核SVM</td>
<td align="center">通道加权</td>
</tr>

<tr>
<td align="center">2008</td>
<td align="center">密集＋兴趣点，多特征</td>
<td align="center">软量化</td>
<td align="center">SPM</td>
<td align="center">多分类器</td>
<td align="center">多模型</td>
</tr>

<tr>
<td align="center">2009</td>
<td align="center">密集 SIFT</td>
<td align="center">GMM，LCC</td>
<td align="center">SPM</td>
<td align="center">线性SVM</td>
<td align="center">多特征</td>
</tr>

<tr>
<td align="center">2010</td>
<td align="center">密集＋兴趣点，多特征</td>
<td align="center">向量量化</td>
<td align="center">SPM，检测</td>
<td align="center">多分类器</td>
<td align="center">多模型</td>
</tr>

<tr>
<td align="center">2011</td>
<td align="center">密集＋兴趣点，多特征</td>
<td align="center">向量量化</td>
<td align="center">SPM，检测</td>
<td align="center">多分类器</td>
<td align="center">多模型</td>
</tr>

<tr>
<td align="center">2012</td>
<td align="center">密集＋兴趣点，多特征</td>
<td align="center">向量量化，Fisher</td>
<td align="center">SPM，检测</td>
<td align="center">多分类器</td>
<td align="center">多模型</td>
</tr>
</tbody>
</table>

<p><br></p>

<h2 id="3-imagenet图像分类算法">#3 <strong>ImageNet图像分类算法</strong></h2>

<table>
<thead>
<tr>
<th align="center">年份</th>
<th align="center">模型</th>
<th align="center">TOP5-ACC</th>
<th align="center">备注</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">2010</td>
<td align="center">密集 HOG LBP+SVM</td>
<td align="center">71.8%</td>
<td align="center"></td>
</tr>

<tr>
<td align="center">2011</td>
<td align="center">Fisher+线性SVM</td>
<td align="center">74.2%</td>
<td align="center"></td>
</tr>

<tr>
<td align="center">2012</td>
<td align="center">AlexNet <sup><a href="#ref02">[2]</a></sup></td>
<td align="center">84.6%</td>
<td align="center">ReLU、image flip、patch extractions、dropout</td>
</tr>

<tr>
<td align="center">2013</td>
<td align="center">ZFNet <sup><a href="#ref03">[3]</a></sup></td>
<td align="center">88.8%</td>
<td align="center">similar architecture to AlexNet、7x7 first filter</td>
</tr>

<tr>
<td align="center">2014</td>
<td align="center">VGGNet <sup><a href="#ref04">[4]</a></sup></td>
<td align="center">92.7%</td>
<td align="center">3x3 sized filters、<font color="red"><em>Leaderboard Ranking 2</em></font></td>
</tr>

<tr>
<td align="center">2014</td>
<td align="center">GoogleNet <sup><a href="#ref05">[5]</a></sup></td>
<td align="center">93.3%</td>
<td align="center">Inception modules</td>
</tr>

<tr>
<td align="center">2015</td>
<td align="center">ResNet <sup><a href="#ref06">[6]</a></sup></td>
<td align="center">96.4%</td>
<td align="center">residual block、Ultra-deep</td>
</tr>
</tbody>
</table>

<p><br></p>

<h2 id="4-关键技术">#4 <strong>关键技术</strong></h2>

<p>&emsp;&emsp;Keywords: <code>HOG</code> <code>SIFT</code> <code>BOF</code> <code>VLAD</code> <code>LeNet</code> <code>AlexNet</code> <code>ZFNet</code> <code>VGGNet</code> <code>GoogleNet</code> <code>ResNet</code> <code>DenseNet</code> <code>ResNeXt</code> <code>SENet</code></p>

<h3 id="4-1-hog-sup-转载-ref07-sup-sup-11-ref11-sup"><strong>4.1 HOG</strong> <sup><a href="#ref07">[转载]</a></sup><sup><a href="#ref11">[11]</a></sup></h3>

<p>&emsp;&emsp;Histogram of oriented gradient, 梯度直方图。将图像分为小的细胞单元(cells)，每个细胞单元计算一个梯度方向(或边缘方向)直方图。为了对光照和阴影有更好的不变性，需要对直方图进行对比度归一化。将检测窗口中的所有块的HOG描述子组合起来就形成了最终的特征向量。</p>

<h4 id="emsp-emsp-1-灰度化-br"><strong>&emsp;&emsp;1）灰度化</strong><br></h4>

<p>&emsp;&emsp;Hog特征提取的是纹理特征，颜色信息不起作用，所以现将彩色图转为灰度图。</p>

<h4 id="emsp-emsp-2-标准化-br"><strong>&emsp;&emsp;2）标准化</strong><br></h4>

<p>&emsp;&emsp;为了提高检测器对光照等干扰因素的鲁棒性，需要对图像进行Gamma校正，以完成对整个图像的归一化，目的是调节图像的对比度，降低局部光照和阴影所造成的影响，同时也可以降低噪音的干扰；（当r取1/2时，像素的取值范围就从0~255变换到0~15.97）。</p>

<h4 id="emsp-emsp-3-计算像素梯度-br"><strong>&emsp;&emsp;3）计算像素梯度</strong><br></h4>

<p>&emsp;&emsp;计算图像像素的梯度：根据下面的公式计算每个像素的水平方向和竖直方向的梯度，并计算每个像素位置的梯度大小和方向。图像在像素点（x,y）处的水平方向和垂直方向的梯度。随后利用所得梯度计算像素点（x,y）处的梯度幅值和梯度方向。
$$ G_{x}(x, y) = G(x+1, y) - G(x-1, y) $$
$$ G_{y}(x, y) = G(x, y+1) - G(x, y-1) $$
<br>
$$ \nabla G(x, y) = \sqrt{G_{x}(x, y)^{2}+G_{y}(x, y)^{2}} $$
$$ \theta (x, y) = tan^{-1}\frac{G_{y}(x, y)}{G_{x}(x, y)} $$</p>

<h4 id="emsp-emsp-4-统计cell直方图-br"><strong>&emsp;&emsp;4）统计cell直方图</strong><br></h4>

<p>&emsp;&emsp;将图像划分成小的Cell，将梯度方向映射到180度的范围内，将像素的梯度幅值作为权值进行投影，用梯度方向决定向哪一维进行投影，假如该像素的梯度方向为20度，梯度幅值为10，那么直方图的第二维就加10。下图是一个细胞单元内的方向梯度直方图，角度分辨率是在180度的范围内，以20度等分，即一个细胞单元的HOG特征是一个9维的向量。

<figure class="center">
    
        <img src="../../img/histogram.png" />
    
    
    <figcaption>
        <h4>一个细胞单元的梯度方向直方图</h4>
        
    </figcaption>
    
</figure>
</p>

<h4 id="emsp-emsp-5-统计block直方图-br"><strong>&emsp;&emsp;5）统计block直方图</strong><br></h4>

<p>&emsp;&emsp;统计每个细胞单元内的梯度直方图，形成每个细胞单元的描述子，由cell组成更大的描述子，称为块，将一个块内四个cell的特征向量串联起来就构成了该块的梯度方向直方图，按照一个细胞单元是9维的HOG特征，则一个块的HOG特征为4x9=36维。由于局部光照的变化，以及前景背景对比度的变化，使得梯度强度的变化范围非常大，这就需要对梯度做局部对比度归一化。这里的策略是针对每个块进行对比度归一化，一般使用L2-norm。</p>

<h4 id="emsp-emsp-6-统计window直方图-br"><strong>&emsp;&emsp;6）统计window直方图</strong><br></h4>

<p>&emsp;&emsp;只需要将窗口内所有块的Hog特征向量串联起来就得到了Window的HOG特征。</p>

<h4 id="emsp-emsp-7-统计图像直方图-br"><strong>&emsp;&emsp;7）统计图像直方图</strong><br></h4>

<p>&emsp;&emsp;一幅图像可以无重叠的划分为多个Window，这时将所有Window的特征向量串联起来就是整幅图像的HOG特征了，如果Window的大小和图像的大小相同，那么Window的HOG特征就是整幅图像的HOG特征。</p>

<h3 id="emsp-emsp-note-that-br"><strong>&emsp;&emsp;<code>NOTE THAT !!!</code></strong><br></h3>

<p>&emsp;&emsp;<em>Cell、Block、stride、细胞单元、块、步长、窗口、图像、Hog特证有什么联系？</em></p>

<p>&emsp;&emsp;其实Cell就是细胞单元，Block指的就是块，Stride是步长，为了方便理解，下面给出了窗口、块、细胞单元、步长的相互关系图。</p>


<figure class="center">
    
        <img src="https://upload-images.jianshu.io/upload_images/3584856-69d37207b9fa542f.png?imageMogr2/auto-orient/" />
    
    
    <figcaption>
        <h4>图像、窗口、块、细胞单元、步长的关系图</h4>
        
    </figcaption>
    
</figure>


<p>&emsp;&emsp;可以看到4个相邻Cell就组成了一个Block（左上角所示），接下来Block在图像上按照一定stride（黄色箭头），按照从左至右、从上至下的顺序进行滑动对图像进行遍历，通过第4步的计算我们就可以得到一个Cell的方向梯度直方图了，那么将四个Cell的方向梯度直方图进行串联，就组成了一个Block的梯度方向直方图，接下来该Block对Window进行遍历，是不是就有许多新的Block了（不断地会有4个相邻的Cell组成新的Block），最后我们将所有Block的方向梯度直方图串联起来就是Window的HOG特征啦，一幅图像可以当作一个窗口，也可以无重叠的划分为多个不重叠的窗口，如果是这种情况，将所有Window的特征串联起来就是图像的HOG特征了。记住我们最后得到的是一个行向量（列向量）。</p>

<h3 id="4-2-sift-sup-8-ref08-sup-sup-12-ref12-sup"><strong>4.2 SIFT</strong> <sup><a href="#ref08">[8]</a></sup><sup><a href="#ref12">[12]</a></sup></h3>

<p>&emsp;&emsp;Scale Invariant Feature Transform, 尺度不变特征变换。是一种检测局部特征的算法，该算法在DoG (Difference of Gaussian)空间上求一幅图中的关键点(特征点)；最后利用HOG方法表征关键点，获得图像特征。除了尺寸不变外，为了具有一定的旋转不变性，每个特征点方向与坐标轴方向保持一致。</p>

<h4 id="emsp-emsp-1-构建尺度空间-br"><strong>&emsp;&emsp;1）构建尺度空间</strong><br></h4>

<p>&emsp;&emsp;SIFT方法的提出主要为了能解决同一图像在不同尺寸下的特征提取问题。构建尺度空间的具体流程如下图，首先针对原图像进行下采样操作（每次采样图像长宽各减半，下图水平方向从左至右scale逐渐变小）；另外，在同一图像尺寸下对图像进行高斯模糊 <sup><a href="#ref09">[9]</a></sup>，获得不同分辨率的图像。</p>


<figure class="center">
    
        <img src="../../img/octave.png" />
    
    
    <figcaption>
        <h4>原图像的尺度空间图</h4>
        
    </figcaption>
    
</figure>


<h4 id="emsp-emsp-2-构建dog尺度空间-br"><strong>&emsp;&emsp;2）构建DoG尺度空间</strong><br></h4>

<p>&emsp;&emsp;SIFT算法为了提取图像中的关键特征，因此将提取图像特征的空间映射到图像的高斯差分（Difference of Gaussian，DoG）空间上（即下图对应的<font color="green">绿色虚线框</font>，<font color="gray">灰色虚线框</font>即为第一小节构建的原图像尺度空间）。高斯差分空间的中对应图像空间的计算公式如下所示。
$$ D(x,y,\sigma) = (G(x,y,k^{n}\sigma) - G(x,y,k^{n-1}\sigma)) * I(x,y) $$</p>


<figure class="center">
    
        <img src="../../img/DoG.png" />
    
    
    <figcaption>
        <h4>DoG尺度空间图</h4>
        
    </figcaption>
    
</figure>


<h4 id="emsp-emsp-3-寻找关键点-br"><strong>&emsp;&emsp;3）寻找关键点</strong><br></h4>

<p>&emsp;&emsp;寻找DoG尺度空间的极值点，每一个采样点和它所有的相邻点比较，看其是否比它的图像域和尺度域的相邻点大或者小。如下图所示，中间的检测点和它同尺度的8个相邻点和上下相邻尺度对应的9×2个点共26个点比较。</p>


<figure class="center">
    
        <img src="../../img/keypoint.png" />
    
    
    <figcaption>
        <h4>DoG尺度空间关键点</h4>
        
    </figcaption>
    
</figure>


<h4 id="emsp-emsp-4-删除噪声点-br"><strong>&emsp;&emsp;4）删除噪声点</strong><br></h4>

<p>&emsp;&emsp;根据任务，比如关注图像拐点信息的任务，可设定阈值筛选关键点梯度。</p>

<h4 id="emsp-emsp-5-生成关键点特征-br"><strong>&emsp;&emsp;5）生成关键点特征</strong><br></h4>

<p>&emsp;&emsp;4X4 cell 统计45°分成的8个方位的幅值，16X16 window一共 可得 (<sup>16</sup>&frasl;<sub>4</sub>)^2 * 8 = 128维的SIFT特征。

<figure class="center">
    
        <img src="../../img/feature-generator.png" />
    
    
    <figcaption>
        <h4>生成关键点特征</h4>
        
    </figcaption>
    
</figure>
</p>

<h3 id="4-3-alexnet-sup-2-ref02-sup"><strong>4.3 AlexNet</strong> <sup><a href="#ref02">[2]</a></sup></h3>

<p>&emsp;&emsp;AlexNet为2012年ImageNet图像分类的冠军网络。该网络的提出标志着卷积神经网络CNN将在计算机视觉领域上取得重大突破。AlexNet的Top5错误率（前5个错误是在给定图像的情况下，该模型未能输出具有前5个预测的正确标签的概率）为15.4%。当时基于<code>手工特征</code>最好的模型Top5错误率达26.2%。AlexNet模型结果图如下。</p>


<figure class="center">
    
        <img src="../../img/AlexNet.png" />
    
    
    <figcaption>
        <h4>AlexNet模型结构图</h4>
        
    </figcaption>
    
</figure>


<p>&emsp;&emsp;AlexNet一共有包含5个卷积层（conv layer）、若干池化层（pooling）、3个全连接层（fully connected layer），并且运用了ReLU激励单元、dropout方法、数据增强方法。主要特点如下。</p>

<ul>
<li>使用的数据增强技术（包括水平翻转和图像片段特征提取）</li>
<li>使用ReLU作为非线性函数（其减少训练时间，ReLU比传统tanh函数快几倍）</li>
<li>使用批量随机梯度下降方法对模型进行训练，同时利用动量和权重衰减技术。</li>
<li>使用dropout方法（利用dropout方法类似于随机森林随机抽取特征，可缓解模型过拟合）</li>
</ul>

<h3 id="4-4-zfnet-sup-3-ref04-sup"><strong>4.4 ZFNet</strong> <sup><a href="#ref04">[3]</a></sup></h3>

<p>&emsp;&emsp;<code>coming soon...</code></p>

<h3 id="4-5-vggnet-sup-4-ref04-sup"><strong>4.5 VGGNet</strong> <sup><a href="#ref04">[4]</a></sup></h3>

<p>&emsp;&emsp;<code>coming soon...</code></p>

<h3 id="4-6-googlenet-sup-5-ref05-sup"><strong>4.6 GoogleNet</strong> <sup><a href="#ref05">[5]</a></sup></h3>

<p>&emsp;&emsp;<code>coming soon...</code></p>

<p><br></p>

<h2 id="参考资料"># <strong>参考资料</strong></h2>

<ol>
<li><a id="ref01"><a href="http://cjc.ict.ac.cn/online/cre/hkq-2014526115913.pdf">图像物体分类与检测算法综述</a></a></li>
<li><a id="ref02"><a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">Imagenet classification with deep convolutional neural networks</a></a></li>
<li><a id="ref03"><a href="https://link.springer.com/chapter/10.1007/978-3-319-10590-1_53">Visualizing and understanding convolutional networks</a></a></li>
<li><a id="ref04"><a href="http://arxiv.org/pdf/1409.1556v6.pdf">Very deep convolutional networks for large-scale image recognition</a></a></li>
<li><a id="ref05"><a href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf">Going deeper with convolutions</a></a></li>
<li><a id="ref06"><a href="https://arxiv.org/pdf/1512.03385v1.pdf">Deep residual learning for image recognition</a></a></li>
<li><a id="ref07"><a href="https://www.jianshu.com/p/6f69c751e9e7">[转载] 方向梯度直方图（HOG）</a></a></li>
<li><a id="ref08"><a href="https://www.jianshu.com/p/6f69c751e9e7">SIFT：Theory and Practice</a></a></li>
<li><a id="ref09"><a href="http://www.ruanyifeng.com/blog/2012/11/gaussian_blur.html">高斯模糊的算法</a></a></li>
<li><a id="ref10"><a href="https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html">The 9 Deep Learning Papers You Need To Know About (Understanding CNNs Part 3)</a></a></li>
<li><a id="ref11"><a href="https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf">Histograms of Oriented Gradients for Human Detection</a></a></li>
<li><a id="ref12"><a href="https://www.cse.unr.edu/~bebis/CS491Y/Papers/Lowe04.pdf">Distinctive Image Features from Scale-Invariant Keypoints</a></a></li>
<li><a id="ref13"><a href="http://ieeexplore.ieee.org/document/5540039/">Aggregating local descriptors into a compact image representation</a></a></li>
<li><a id="ref14"><a href="http://www.europe.naverlabs.com/layout/set/print/content/download/20837/148502/file/2006-034.pdf">Fisher kernels on visual vocabularies for image categorization</a></a></li>
<li><a id="ref15"><a href="http://mplab.ucsd.edu/~marni/Igert/Lazebnik_06.pdf">Beyond Bags of Features: Spatial Pyramid Matchingfor Recognizing Natural Scene Categories</a></a></li>
</ol>

<p><br></p>
    </div>

    
    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">shield</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">2018-03-26</span>
  </p>
  
  
</div>

    
    

    <footer class="post-footer">
      <div class="post-tags">
          
          <a href="/tags/image-cls/">image-cls</a>
          
        </div>

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/trends/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">[Pin] Keep up with New Trends</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/python-manual/">
            <span class="next-text nav-default">Python Manual</span>
            <span class="prev-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        
  

  

  
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="https://github.com/shieldOnTheRoad" class="iconfont icon-github" title="github"></a>
      <a href="http://localhost:1313" class="iconfont icon-weibo" title="weibo"></a>
      <a href="https://www.zhihu.com/people/lin-jiong-jiong-45/activities" class="iconfont icon-zhihu" title="zhihu"></a>
  <a href="http://shieldOnTheRoad.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  

  <span class="copyright-year">
    &copy; 
    2018
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">shield</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
<script src="/lib/highlight/highlight.pack.js?v=20171001"></script><script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>
  <script type="text/javascript" src="/lib/fancybox/jquery.fancybox-3.1.20.min.js"></script>
<script type="text/javascript" src="/dist/even.min.js?v=3.0.0"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      showProcessingMessages: false,
      messageStyle: 'none'
    });
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"></script>








</body>
</html>
